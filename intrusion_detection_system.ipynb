{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection System using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds an intrusion detection system using the **NSL-KDD dataset**, a refined version of the KDD'99 dataset. The goal is to build a network intrusion detector, a predictive model capable of distinguishing between 'bad' connections (intrusions or attacks) and 'good' normal connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Train_data.csv')\n",
    "test_df = pd.read_csv('Test_data.csv')\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Testing data shape:\", test_df.shape)\n",
    "\n",
    "# The test set has a garbage column at the end, let's fix the column names\n",
    "train_cols = train_df.columns\n",
    "if len(test_df.columns) == len(train_cols):\n",
    "    test_df.columns = train_cols\n",
    "    test_df.drop('class', axis=1, inplace=True)\n",
    "else:\n",
    "    test_df.columns = train_cols[:-1]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='class', data=train_df, palette='viridis')\n",
    "plt.title('Distribution of Connection Types (Class)', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('class', axis=1)\n",
    "y = train_df['class']\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "X_all = pd.concat([X, test_df], axis=0)\n",
    "X_all = pd.get_dummies(X_all, columns=categorical_features, drop_first=True)\n",
    "\n",
    "X_processed = X_all[:len(X)]\n",
    "test_processed = X_all[len(X):]\n",
    "\n",
    "train_cols = X_processed.columns\n",
    "test_cols = test_processed.columns\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    test_processed[c] = 0\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X_processed[c] = 0\n",
    "test_processed = test_processed[train_cols]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_val[numerical_features] = scaler.transform(X_val[numerical_features])\n",
    "test_processed[numerical_features] = scaler.transform(test_processed[numerical_features])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model (on Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report on Validation Set:\")\n",
    "print(classification_report(y_val_encoded, y_pred_val, target_names=le.classes_))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = confusion_matrix(y_val_encoded, y_pred_val)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix on Validation Set', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a few samples from the validation set\n",
    "n_samples = 10\n",
    "sample_df = X_val.head(n_samples).copy()\n",
    "\n",
    "# Get true labels\n",
    "true_labels_encoded = y_val_encoded[:n_samples]\n",
    "true_labels = le.inverse_transform(true_labels_encoded)\n",
    "sample_df['Actual Class'] = true_labels\n",
    "\n",
    "# Make predictions\n",
    "predictions_encoded = model.predict(X_val.head(n_samples))\n",
    "predictions = le.inverse_transform(predictions_encoded)\n",
    "sample_df['Predicted Class'] = predictions\n",
    "\n",
    "print(f\"Showing predictions for the first {n_samples} samples of the validation set:\")\n",
    "sample_df[['Actual Class', 'Predicted Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Model and Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'intrusion_detection_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "joblib.dump(X_train.columns, 'model_columns.pkl')\n",
    "print(\"Model, scaler, label encoder, and columns saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and preprocessors\n",
    "loaded_model = joblib.load('intrusion_detection_model.pkl')\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "loaded_le = joblib.load('label_encoder.pkl')\n",
    "loaded_columns = joblib.load('model_columns.pkl')\n",
    "\n",
    "# Example: Take one sample from the processed test set\n",
    "sample = test_processed.iloc[[0]]\n",
    "\n",
    "# Make prediction\n",
    "prediction_encoded = loaded_model.predict(sample[loaded_columns])\n",
    "prediction = loaded_le.inverse_transform(prediction_encoded)\n",
    "\n",
    "print(f\"Prediction for the first sample in the test set: {prediction[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
